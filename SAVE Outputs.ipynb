{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "executionInfo": {
     "elapsed": 886,
     "status": "ok",
     "timestamp": 1612881125722,
     "user": {
      "displayName": "Juan Sanguineti",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-XfQmXcnLZFwq9qN4wdR7tl7YcAnxPy69Qje2Ug=s64",
      "userId": "06510006363156190395"
     },
     "user_tz": 180
    },
    "id": "zyvGUVHeJdY1"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import sqlalchemy_hana\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import datetime\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect a Hana\n",
    "\n",
    "def connectToHANA():\n",
    "    try:\n",
    "        conn = sqlalchemy.create_engine(\n",
    "            'hana://DBADMIN:HANAtest2908@8969f818-750f-468f-afff-3dc99a6e805b.hana.trial-us10.hanacloud.ondemand.com:443/?encrypt=true&validateCertificate=false').connect()\n",
    "    except Exception as e:\n",
    "        print('Connection failed! ' + str(e))\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db_from_SAGE(uploadToHana):\n",
    "\n",
    "    table_urls = {'BOM': r'http://10.4.240.65/api/IntegrationAPI/GetBOM',\n",
    "              'Inventory': r'http://10.4.240.65/api/IntegrationAPI/GetInventory',\n",
    "              'Facility': r'http://10.4.240.65/api/IntegrationAPI/GetItemFacility',\n",
    "              'ItemMaster': r'http://10.4.240.65/api/IntegrationAPI/GetItemMstr',\n",
    "              'RoutingAndRates': r'http://10.4.240.65/api/IntegrationAPI/GetRoutingAndRates',\n",
    "              'WorkCenters': r'http://10.4.240.65/api/IntegrationAPI/GetWorkCenters',\n",
    "              'WorkOrders': r'http://10.4.240.65/api/IntegrationAPI/GetWorkOrders'}\n",
    "\n",
    "    for table in table_urls:\n",
    "        try:\n",
    "            globals()[table] = pd.read_json(table_urls[table])\n",
    "            print(f'Table {table} succesfully loaded.')\n",
    "        except Exception as e:\n",
    "            print(f'Couldn\\'t load table {table}: ' + str(e))\n",
    "\n",
    "    if uploadToHana:\n",
    "        connection = connectToHANA()\n",
    "        for table in table_urls:\n",
    "            try:\n",
    "                connection.execute(f'DELETE FROM {table}')\n",
    "                globals()[table].to_sql(table.lower(), schema = 'ANYLOGIC', con = connection, if_exists = 'append', index = False)\n",
    "                print(f'Table {table} was uploaded to HANA succesfully.')\n",
    "            except Exception as e:\n",
    "                print(f'Couldn\\'t save {table} table into HANA. ' + str(e))\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table BOM succesfully loaded.\n",
      "Table Inventory succesfully loaded.\n",
      "Table Facility succesfully loaded.\n",
      "Table ItemMaster succesfully loaded.\n",
      "Table RoutingAndRates succesfully loaded.\n",
      "Table WorkCenters succesfully loaded.\n",
      "Table WorkOrders succesfully loaded.\n",
      "Table BOM was uploaded to HANA succesfully.\n",
      "Table Inventory was uploaded to HANA succesfully.\n",
      "Table Facility was uploaded to HANA succesfully.\n",
      "Table ItemMaster was uploaded to HANA succesfully.\n",
      "Table RoutingAndRates was uploaded to HANA succesfully.\n",
      "Table WorkCenters was uploaded to HANA succesfully.\n",
      "Table WorkOrders was uploaded to HANA succesfully.\n"
     ]
    }
   ],
   "source": [
    "update_db_from_SAGE(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hana output\n",
    "out_due_date_backlog = pd.read_sql('SELECT * FROM \"OUTPUT\".\"OUT_DUE_DATE_BACKLOG\"', con=connectToHANA())\n",
    "schedule_bulk = pd.read_sql('SELECT * FROM \"OUTPUT\".\"SCHEDULE_BULK\"', con=connectToHANA())\n",
    "schedule_sku = pd.read_sql('SELECT * FROM \"OUTPUT\".\"SCHEDULE_SKU\"', con=connectToHANA())\n",
    "\n",
    "#sage\n",
    "itemmaster = pd.read_sql('SELECT * FROM \"SAGE\".\"ITEMMASTER\"', con=connectToHANA())\n",
    "workorders = pd.read_sql('SELECT * FROM \"SAGE\".\"WORKORDERS\"', con=connectToHANA())\n",
    "\n",
    "#anylogic\n",
    "bulk_inventory = pd.read_sql('SELECT * FROM \"ANYLOGIC\".\"BULK_INVENTORY\"', con=connectToHANA())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toma la week de hoy y le suma 3, queda una mas de la que viene de JD Edwards\n",
    "\n",
    "def week():\n",
    "    date = datetime.date.today()\n",
    "    year, week_num, day_of_week = date.isocalendar()\n",
    "    week = 'Week ' + str(week_num + 3)\n",
    "    return week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1612881132718,
     "user": {
      "displayName": "Juan Sanguineti",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-XfQmXcnLZFwq9qN4wdR7tl7YcAnxPy69Qje2Ug=s64",
      "userId": "06510006363156190395"
     },
     "user_tz": 180
    },
    "id": "cmtOfCgCKtvm"
   },
   "outputs": [],
   "source": [
    "#creates extruders output for SAC\n",
    "\n",
    "def extruders(schedule_bulk):\n",
    "    \n",
    "    #copy dataframe\n",
    "    extruders = schedule_bulk.copy()\n",
    "\n",
    "    #drop columns\n",
    "    extruders.drop(['week', \n",
    "                    'waste', \n",
    "                    'entity', \n",
    "                    'seed', \n",
    "                    'machine code'\n",
    "                    ], axis=1, inplace = True)\n",
    "   \n",
    "    #rename colums\n",
    "    dictionary = {\"original due date\": \"Original Due Date\",\n",
    "                  \"ending hour\": \"Ending Hour\",\n",
    "                  \"starting hour\": \"Starting Hour\",\n",
    "                  \"extruder sequence\": \"Sequence\",\n",
    "                  \"run\":\"Run\", \n",
    "                  \"machine\" : \"WorkCenter\",\n",
    "                  \"starting\": \"Starting Date\",\n",
    "                  \"ending\": \"Ending Date\",\n",
    "                  \"prod planned\": \"Demand\", \n",
    "                  \"prod att\": \"Production\",\n",
    "                  \"shrinkage\": \"Shrinkage\",\n",
    "                  \"hours\": \"Hours\",\n",
    "                  \"fg assigned\": \"ItemRef\",\n",
    "                  \"last change over\":\"CO Time\",\n",
    "                  \"stuck time\" : \"Waiting Time\", \n",
    "                  \"bulk code\":\"ItemNumber\",\n",
    "                  \"purchase order\": \"Purchase Order\",\n",
    "                  \"sales order\":\"Sales Order\",\n",
    "                  \"work order\":\"Work Order Ref\"}\n",
    "    extruders.rename(columns= dictionary ,inplace=True)\n",
    "    \n",
    "    #fill nan with 0\n",
    "    extruders = extruders.fillna('0')\n",
    "\n",
    "    #insert version, entity, process date, categorycode\n",
    "        #extruders['Version'] = 'SIM'\n",
    "        #extruders['Entity'] = 'Alphia'\n",
    "    extruders['Process Date'] = week()\n",
    "    extruders['CategoryCode'] = \"INT\"\n",
    "    \n",
    "    #change data types to int\n",
    "    extruders[\"Production\"] = extruders[\"Production\"].astype(float).astype(int)\n",
    "    extruders[\"Demand\"] = extruders[\"Demand\"].astype(float).astype(int)\n",
    "    \n",
    "    #from str to floats\n",
    "    extruders[\"Shrinkage\"] = extruders[\"Shrinkage\"].astype(float)\n",
    "    extruders[\"CO Time\"] = extruders[\"CO Time\"].astype(float)\n",
    "    extruders[\"Hours\"] = extruders[\"Hours\"].astype(float)\n",
    "    extruders[\"Waiting Time\"] = extruders[\"Waiting Time\"].astype(float)\n",
    "\n",
    "    #keep only dates of timestamp\n",
    "    extruders[\"Ending Date\"] = extruders[\"Ending Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "    extruders[\"Starting Date\"] = extruders[\"Starting Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "    extruders[\"Original Due Date\"] = extruders[\"Original Due Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "\n",
    "    #convert str of dates to datetime\n",
    "        #extruders[\"Starting Date\"] = pd.to_datetime(extruders[\"Starting Date\"])\n",
    "        #extruders[\"Ending Date\"] = pd.to_datetime(extruders[\"Ending Date\"])\n",
    "        #extruders[\"Original Due Date\"] = pd.to_datetime(extruders[\"Original Due Date\"])\n",
    "\n",
    "    #replace missing with 0\n",
    "    extruders = extruders.replace('missing','0')\n",
    "    \n",
    "    #round \n",
    "    extruders = extruders.round(1)\n",
    "    \n",
    "    #return dataframe\n",
    "    return extruders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inventory\n",
    "\n",
    "def inventory(bulk_inventory, extruders):\n",
    "    \n",
    "    #copy df\n",
    "    bulk_inventory_copy = bulk_inventory.copy()\n",
    "    \n",
    "    #rename columns\n",
    "    bulk_inventory_copy.rename(columns = {\n",
    "                                    'bulk':'ItemNumber',\n",
    "                                    'due date': 'Due Date',\n",
    "                                    'location':'Facility',\n",
    "                                    'work order':'Work Order',\n",
    "                                    'purchase order': 'Purchase Order',\n",
    "                                    'inventory in pounds': 'Inventory'},\n",
    "                                    inplace = True)\n",
    "    \n",
    "    #drop\n",
    "    bulk_inventory_copy.drop(['is_diet'], inplace = True, axis=1)\n",
    "    \n",
    "    #process date y run\n",
    "    bulk_inventory_copy['Process Date'] = week()\n",
    "    bulk_inventory_copy['Run'] = extruders.loc[0,\"Run\"]\n",
    "    \n",
    "    #category code\n",
    "    bulk_inventory_copy['CategoryCode'] = \"INT Inventory\"\n",
    "\n",
    "    #return\n",
    "    return bulk_inventory_copy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates packline output for SAC\n",
    "\n",
    "def packlines(schedule_sku, extruders):\n",
    "        \n",
    "    #create a copy of the df\n",
    "    packlines = schedule_sku.copy()\n",
    "    \n",
    "\n",
    "    \n",
    "    # drop columns\n",
    "    packlines.drop(['order demand pounds',\n",
    "                    'production demand bags',\n",
    "                    'sku description',\n",
    "                    'machine code',\n",
    "                    'inventory demand bags', \n",
    "                    'entity', \n",
    "                    'seed',\n",
    "                    'machine code'], axis=1, inplace = True)\n",
    "\n",
    "    # rename columns\n",
    "    dictionary = {\"ending hour\": \"Ending Hour\", \n",
    "                  \"starting hour\": \"Starting Hour\", \n",
    "                  \"starting date\": \"Starting Date\", \n",
    "                  \"ending date\": \"Ending Date\", \n",
    "                  \"original due date\": \"Original Due Date\",\n",
    "                  \"machines sequences\":\"Sequence\",\n",
    "                  \"run\": \"Run\", \n",
    "                  \"sku\": \"ItemNumber\", \n",
    "                  \"formula\": \"Diet\", \n",
    "                  \"machine\": \"WorkCenter\", \n",
    "                  \"production demand pounds\": \"Demand\", \n",
    "                  \"demand attained pounds\": \"Production\", \n",
    "                  \"order demand bags\": \"Bags Demand\", \n",
    "                  \"demand attained bags\": \"Bags Production\", \n",
    "                  \"hours\": \"Hours\",\n",
    "                  \"purchase order\": \"Purchase Order\",\n",
    "                  \"sales order\":\"Sales Order\",\n",
    "                  \"work order\":\"Work Order\",\n",
    "                  \"inventory demand pounds\": \"Inventory\",\n",
    "                  }\n",
    "    packlines.rename(columns = dictionary, inplace=True)\n",
    "    \n",
    "    #fill nan with 0\n",
    "    packlines = packlines.fillna('0')\n",
    "    \n",
    "    #change data type to int\n",
    "    packlines[\"Bags Production\"] = packlines[\"Bags Production\"].astype(float).astype(int)\n",
    "    packlines[\"Bags Demand\"] = packlines[\"Bags Demand\"].astype(float).astype(int)\n",
    "    packlines[\"Demand\"] = packlines[\"Demand\"].astype(float).astype(int)\n",
    "    packlines[\"Production\"] = packlines[\"Production\"].astype(float).astype(int)\n",
    "    packlines[\"Inventory\"] = packlines[\"Inventory\"].astype(float).astype(int)\n",
    "\n",
    "    #change to float\n",
    "    packlines[\"Hours\"] = packlines[\"Hours\"].astype(float)\n",
    "    \n",
    "    #round floats\n",
    "    packlines = packlines.round(1)\n",
    "    \n",
    "    #insert version, entity, process date, CategoryCode\n",
    "        #packlines[\"Version\"] = \"SIM\"\n",
    "    packlines[\"Process Date\"] = week()\n",
    "        #packlines['Entity'] = 'Alphia'\n",
    "    packlines['CategoryCode'] = \"FG\"\n",
    "\n",
    "    #keep only dates of timestamp\n",
    "    packlines[\"Starting Date\"] = packlines[\"Starting Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "    packlines[\"Ending Date\"] = packlines[\"Ending Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "    packlines[\"Original Due Date\"] = packlines[\"Original Due Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "\n",
    "    #convert str of dates to datetime\n",
    "        #packlines[\"Starting Date\"] = pd.to_datetime(packlines[\"Starting Date\"])\n",
    "        #packlines[\"Ending Date\"] = pd.to_datetime(packlines[\"Ending Date\"])\n",
    "        #packlines[\"Original Due Date\"] = pd.to_datetime(packlines[\"Original Due Date\"])\n",
    "    \n",
    "    #replace missing with 0\n",
    "    packlines = packlines.replace('missing','0')\n",
    "    \n",
    "    #add run to packlines\n",
    "    packlines['Run'] = extruders.loc[0,\"Run\"]\n",
    "\n",
    "    #return dataframe\n",
    "    return packlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack for sac\n",
    "\n",
    "def unpacked(out_due_date_backlog, extruders):    \n",
    "    \n",
    "    #copy table\n",
    "    out_due_date_backlog_copy = out_due_date_backlog.copy()\n",
    "\n",
    "    #fill nan\n",
    "    out_due_date_backlog_copy.fillna('0', inplace=True)\n",
    "    \n",
    "    #insert version, date, week, run\n",
    "    #out_due_date_backlog_copy['Version'] = 'SIM'\n",
    "    out_due_date_backlog_copy['Process Date'] = week()\n",
    "    out_due_date_backlog_copy['run'] = extruders.loc[0,\"Run\"]\n",
    "    #out_due_date_backlog_copy['Entity'] = 'CJ Foods'\n",
    "    \n",
    "    #rename\n",
    "    out_due_date_backlog_copy.rename(columns={\n",
    "            \"run\": \"Run\", \n",
    "            'finished good': 'ItemNumber', \n",
    "            'due date':'Original Due Date',\n",
    "            'location': 'Facility',\n",
    "            'amount unpacked': 'Unpacked Amount',\n",
    "            'work order':'Work Order',\n",
    "            'purchase order':'Purchase Order'\n",
    "            }, inplace=True)\n",
    "\n",
    "    #category code for unpacked\n",
    "    out_due_date_backlog_copy['CategoryCode'] = \"FG Unpacked\"\n",
    "\n",
    "    #attribute name\n",
    "    out_due_date_backlog_copy.name = 'UNPACKED_SAC'\n",
    "    \n",
    "    #return unpack\n",
    "    return out_due_date_backlog_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create unified table of packlines and extrusion that will be uploaded to Hana\n",
    "\n",
    "def unified_sac(packlines, extruders, inventory, unpacked):\n",
    "    \n",
    "    #append one to the other\n",
    "    unified_table = extruders.append([packlines, \n",
    "                                     inventory, \n",
    "                                     unpacked], \n",
    "                                     ignore_index=True)\n",
    "\n",
    "    #fill nulls with 0\n",
    "    unified_table.fillna('0', inplace=True)\n",
    "    \n",
    "    #name attribute\n",
    "    unified_table.name = 'UNIFIED_SAC'\n",
    "\n",
    "    #return unified table\n",
    "    return unified_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create WO demand planning of SAGE to create the unassigned workorders\n",
    "\n",
    "def wo_bulk(itemmaster,workorders):\n",
    "    \n",
    "    #create a copy\n",
    "    ItemMaster_copy = itemmaster.copy()\n",
    "    WorkOrders_copy = workorders.copy()\n",
    "    \n",
    "    #filter order status = 1 for Workorders\n",
    "    filter1 = WorkOrders_copy['OrderStatus'] == '1'\n",
    "    WorkOrders_copy = WorkOrders_copy[filter1] \n",
    "    \n",
    "    #keep some columns of WO\n",
    "    WorkOrders_copy = WorkOrders_copy[['WorkOrderNumber', \n",
    "                             'Purchase_Order',\n",
    "                             'ItemNumber',\n",
    "                             'PlannedQty',\n",
    "                             'CompletedQty',\n",
    "                             'WorkCenter',\n",
    "                             'Operation',\n",
    "                             'OrderStatus',\n",
    "                             'PlannedStart',\n",
    "                             'PlannedEnd'\n",
    "                             ]]\n",
    "    \n",
    "    #rename columns\n",
    "    WorkOrders_copy.rename(columns= {'Purchase_Order':'Purchase Order'} ,inplace=True)\n",
    "\n",
    "    \n",
    "    #keep only dates\n",
    "    WorkOrders_copy[\"PlannedStart\"] = WorkOrders_copy[\"PlannedStart\"].str.split(\"T\", n = 1, expand = True)[0]\n",
    "    WorkOrders_copy[\"PlannedEnd\"] = WorkOrders_copy[\"PlannedEnd\"].str.split(\"T\", n = 1, expand = True)[0]\n",
    "\n",
    "    #filter itemmaster per categorycode = INT\n",
    "    filter2 = ItemMaster_copy['CategoryCode'] == 'INT'\n",
    "    ItemMaster_copy = ItemMaster_copy[filter2]\n",
    "    \n",
    "    #merge dataframes\n",
    "    merge = WorkOrders_copy.merge(ItemMaster_copy[['ItemNumber', \n",
    "                                                   'CategoryCode', \n",
    "                                                   'ItemWeight']], \n",
    "                                  on='ItemNumber',\n",
    "                                  how = 'inner')\n",
    "\n",
    "    #fill null values with 0\n",
    "    merge.fillna('0', inplace=True)\n",
    "    \n",
    "    #from bale to pounds\n",
    "    merge[\"PlannedQty\"] = merge[\"PlannedQty\"].astype(float) * merge[\"ItemWeight\"].astype(float)\n",
    "    merge[\"CompletedQty\"] = merge[\"CompletedQty\"].astype(float) * merge[\"ItemWeight\"].astype(float)\n",
    "    \n",
    "    #float to int\n",
    "    merge[\"PlannedQty\"] = merge[\"PlannedQty\"].astype(int) \n",
    "    merge[\"CompletedQty\"] = merge[\"CompletedQty\"].astype(int)\n",
    "    \n",
    "    #round decimals\n",
    "    merge = merge.round(1)\n",
    "    \n",
    "    #name \n",
    "    merge.name = \"WO_BULK_DEMAND\"\n",
    "    \n",
    "    #return dataframe\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped itemnumbers per uninterrupted sequence and workcenter\n",
    "\n",
    "def group_extruders(extruders, inventory):\n",
    "\n",
    "    #create copy\n",
    "    group_extruders = extruders.copy()\n",
    "    inventory_copy = inventory.copy()\n",
    "\n",
    "    #inventory merge\n",
    "    group_extruders = group_extruders.merge(inventory_copy[['ItemNumber', 'Inventory']], \n",
    "                        on='ItemNumber', \n",
    "                        how = 'left')\n",
    "    #drop columns\n",
    "    group_extruders.drop(['Shrinkage', 'CO Time','Waiting Time'], axis=1, inplace = True)\n",
    "\n",
    "    #change sequence data type and sort the df by WC and sequence\n",
    "    group_extruders['Sequence'] = group_extruders['Sequence'].astype(int)\n",
    "    group_extruders = group_extruders.sort_values(['WorkCenter','Sequence'])\n",
    "    group_extruders['Sequence'] = group_extruders['Sequence'].astype(str)\n",
    "\n",
    "    #keep first colum of group to keep start date\n",
    "    first = group_extruders[['ItemNumber',\n",
    "                             'Sequence', \n",
    "                             'Purchase Order',\n",
    "                             'Starting Date', \n",
    "                             'Starting Hour',\n",
    "                             'Inventory',\n",
    "                             'Run',\n",
    "                             'Process Date',\n",
    "                             'WorkCenter'\n",
    "                            ]].groupby(by=[(\n",
    "                            group_extruders.ItemNumber!=group_extruders.ItemNumber.shift()).cumsum(),\n",
    "                            'ItemNumber'\n",
    "                            ], \n",
    "                            as_index=False).nth([0]).reset_index(drop=True)\n",
    "\n",
    "    #keep last row of group to keep end date\n",
    "    last = group_extruders[['ItemNumber', \n",
    "                            'Ending Date', \n",
    "                            'Ending Hour'\n",
    "                           ]].groupby(by=[(\n",
    "                            group_extruders.ItemNumber!=group_extruders.ItemNumber.shift()).cumsum(),\n",
    "                            'ItemNumber'\n",
    "                            ], as_index=False).nth([-1]).reset_index(drop=True)\n",
    "\n",
    "    #merge first and last rows of group to have the first and last date\n",
    "    firstlast = pd.merge(first, last, how='inner', on='ItemNumber', left_index=True, right_index=True)\n",
    "\n",
    "    #sum all measures of group\n",
    "    suma = group_extruders.groupby(by=[(\n",
    "                            group_extruders.ItemNumber!=group_extruders.ItemNumber.shift()).cumsum(),\n",
    "                            'ItemNumber'\n",
    "                            ], as_index=False).sum()\n",
    "\n",
    "    #merge measures with last and first rows\n",
    "    merge = pd.merge(firstlast, suma, how='inner', on='ItemNumber', left_index=True, right_index=True)\n",
    "\n",
    "    #fill null with 0\n",
    "    merge.fillna(0, inplace=True)\n",
    "\n",
    "    #round decimals\n",
    "    merge = merge.round(1)\n",
    "\n",
    "    #return df\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigned wo to group extrusion\n",
    "\n",
    "def assigned_wo(group_extruders, wo_bulk):\n",
    "    \n",
    "    #create a copu\n",
    "    group_extruders_copy = group_extruders.copy()\n",
    "    wo_bulk_copy = wo_bulk.copy()\n",
    "\n",
    "    #sort key values\n",
    "    group_extruders_copy = group_extruders_copy.sort_values('Production')\n",
    "    wo_bulk_copy = wo_bulk_copy.sort_values('PlannedQty')\n",
    "\n",
    "    #merge as of\n",
    "    merge = pd.merge_asof(\n",
    "                      group_extruders_copy, \n",
    "                      wo_bulk_copy[['PlannedQty', \n",
    "                                    'ItemNumber', \n",
    "                                    'WorkOrderNumber']], \n",
    "                      left_on =\"Production\", \n",
    "                      right_on = 'PlannedQty', \n",
    "                      direction = 'nearest',\n",
    "                      by = 'ItemNumber')\n",
    "    \n",
    "    #create boolean threshold on resta\n",
    "    merge['resta'] = (merge['Production'] / merge['PlannedQty']) * 100\n",
    "    merge['resta'] = merge['resta'].between(95, 105)\n",
    "    \n",
    "    #conditional threshold to set values\n",
    "    merge['Close/Distant Assigned'] = merge['resta'].map({True: 'Close', False: 'Distant'})\n",
    "        \n",
    "    #drop resta\n",
    "    merge.drop(['resta'], axis=1, inplace = True)\n",
    "    \n",
    "    #rename\n",
    "    merge.rename({'WorkOrderNumber':'WorkOrder Assigned'}, inplace = True, axis = 1)\n",
    "    \n",
    "    #create time column\n",
    "    merge['Starting Time'] = merge['Starting Date'] + ' ' + merge['Starting Hour']\n",
    "    merge['Ending Time'] = merge['Ending Date'] + ' ' + merge['Ending Hour']\n",
    "\n",
    "    #attribute name\n",
    "    merge.name = \"GROUPE_EXTRUDERS_ASSIGNED_SAC\"\n",
    "\n",
    "    #return merge\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for unified df \n",
    "\n",
    "extruders = extruders(schedule_bulk)\n",
    "packlines = packlines(schedule_sku, extruders)\n",
    "unpacked = unpacked(out_due_date_backlog, extruders)\n",
    "inventory = inventory(bulk_inventory, extruders)\n",
    "unified_sac = unified_sac(packlines, extruders, inventory, unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables for assigned wo\n",
    "\n",
    "wo_bulk = wo_bulk(itemmaster,workorders)\n",
    "group_extruders = group_extruders(extruders, inventory)\n",
    "assigned_wo = assigned_wo(group_extruders, wo_bulk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sube la lista de tablas de SACA a Hana y pisa segun week y run\n",
    "\n",
    "\n",
    "def upload_output_to_hana():\n",
    "    \n",
    "    #lista de las tablas a subir\n",
    "    lista_tablas_para_SAC = [assigned_wo, unified_sac]\n",
    "    \n",
    "    #coneccion a variable\n",
    "    connection = connectToHANA()\n",
    "    \n",
    "    #itera sobre las tablas, pisa segun run y process date. Si no funciona, dale error\n",
    "    for table in lista_tablas_para_SAC:\n",
    "        \n",
    "        #check whether the table is empty\n",
    "        if len(table.index)==0:\n",
    "             print(table.name +' is empty')\n",
    "            \n",
    "        #if it is not, upload\n",
    "        else:\n",
    "            try:\n",
    "                \n",
    "                #variables de run y process date\n",
    "                Run = table.loc[0,\"Run\"]\n",
    "                Process_Date = table.loc[0,\"Process Date\"]\n",
    "\n",
    "                #execute sql to delete rows on database based on run and process date\n",
    "                connection.execute(f\"\"\"DELETE FROM \"SAC_OUTPUT\".\"{table.name}\" WHERE \"Process Date\" = '{Process_Date}' and \"Run\" = '{Run}'\"\"\")\n",
    "                print('Values deleted succesfully')\n",
    "\n",
    "                #append dataframe to the table\n",
    "                table.to_sql(table.name.lower(), schema='SAC_OUTPUT', con=connection, if_exists='append', index=False)\n",
    "                print(table.name + ' uploaded succesfully')\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                #print problems\n",
    "                print(table.name +' failed to upload! ' + str(e))\n",
    "           \n",
    "    #close hana connection\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values deleted succesfully\n",
      "GROUPE_EXTRUDERS_ASSIGNED_SAC uploaded succesfully\n",
      "Values deleted succesfully\n",
      "UNIFIED_SAC uploaded succesfully\n"
     ]
    }
   ],
   "source": [
    "upload_output_to_hana()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only get csv from df\n",
    "\n",
    "lista_tablas_para_SAC = [wo_bulk]\n",
    "for table in lista_tablas_para_SAC:\n",
    "    table.to_csv(f'{table.name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyRr90otIVsyMnSYvxrxzq",
   "name": "Save output to hana.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
