{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "executionInfo": {
     "elapsed": 886,
     "status": "ok",
     "timestamp": 1612881125722,
     "user": {
      "displayName": "Juan Sanguineti",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-XfQmXcnLZFwq9qN4wdR7tl7YcAnxPy69Qje2Ug=s64",
      "userId": "06510006363156190395"
     },
     "user_tz": 180
    },
    "id": "zyvGUVHeJdY1"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import sqlalchemy_hana\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import datetime\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "pd.set_option('display.max_rows', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect a Hana\n",
    "\n",
    "def connectToHANA():\n",
    "    try:\n",
    "        conn = sqlalchemy.create_engine(\n",
    "            'hana://DBADMIN:HANAtest2908@8969f818-750f-468f-afff-3dc99a6e805b.hana.trial-us10.hanacloud.ondemand.com:443/?encrypt=true&validateCertificate=false').connect()\n",
    "    except Exception as e:\n",
    "        print('Connection failed! ' + str(e))\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_db_from_SAGE():\n",
    "\n",
    "    #tables dict\n",
    "    table_urls = {'BOM': r'http://10.4.240.65/api/IntegrationAPI/GetBOM',\n",
    "              'Inventory': r'http://10.4.240.65/api/IntegrationAPI/GetInventory',\n",
    "              'Facility': r'http://10.4.240.65/api/IntegrationAPI/GetItemFacility',\n",
    "              'ItemMaster': r'http://10.4.240.65/api/IntegrationAPI/GetItemMstr',\n",
    "              'RoutingAndRates': r'http://10.4.240.65/api/IntegrationAPI/GetRoutingAndRates',\n",
    "              'WorkCenters': r'http://10.4.240.65/api/IntegrationAPI/GetWorkCenters',\n",
    "              'WorkOrders': r'http://10.4.240.65/api/IntegrationAPI/GetWorkOrders'}\n",
    "\n",
    "    #tables to df\n",
    "    for table in table_urls:\n",
    "        try:\n",
    "            globals()[table] = pd.read_json(table_urls[table])\n",
    "            print(f'Table {table} succesfully loaded.')\n",
    "        except Exception as e:\n",
    "            print(f'Couldn\\'t load table {table}: ' + str(e))\n",
    "\n",
    "    #connectio to hana\n",
    "    connection = connectToHANA()\n",
    "    \n",
    "    #upload df to hana\n",
    "    for table in table_urls:\n",
    "        try:\n",
    "            connection.execute(f'DELETE FROM \"SAGE\".{table}')\n",
    "            globals()[table].to_sql(table.lower(), schema = 'SAGE', con = connection, if_exists = 'append', index = False)\n",
    "            print(f'Table {table} was uploaded to HANA succesfully.')\n",
    "        except Exception as e:\n",
    "            print(f'Couldn\\'t save {table} table into HANA. ' + str(e))\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOM.to_csv(\"BOM.csv\")\n",
    "Inventory.to_csv(\"Inventory.csv\")  \n",
    "Facility.to_csv(\"Facility.csv\")  \n",
    "ItemMaster.to_csv(\"ItemMaster.csv\")  \n",
    "RoutingAndRates.to_csv(\"RoutingAndRates.csv\")  \n",
    "WorkOrders.to_csv(\"WorkOrders.csv\")\n",
    "WorkCenters.to_csv(\"GetWorkCenters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hana output\n",
    "out_due_date_backlog = pd.read_sql('SELECT * FROM \"OUTPUT\".\"OUT_DUE_DATE_BACKLOG\"', con=connectToHANA())\n",
    "schedule_bulk = pd.read_sql('SELECT * FROM \"OUTPUT\".\"SCHEDULE_BULK\"', con=connectToHANA())\n",
    "schedule_sku = pd.read_sql('SELECT * FROM \"OUTPUT\".\"SCHEDULE_SKU\"', con=connectToHANA())\n",
    "\n",
    "#sage Hana\n",
    "itemmaster = pd.read_sql('SELECT * FROM \"SAGE\".\"ITEMMASTER\"', con=connectToHANA())\n",
    "workorders = pd.read_sql('SELECT * FROM \"SAGE\".\"WORKORDERS\"', con=connectToHANA())\n",
    "\n",
    "#anylogic\n",
    "bulk_inventory = pd.read_sql('SELECT * FROM \"ANYLOGIC\".\"BULK_INVENTORY\"', con=connectToHANA())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toma la week de hoy y le suma 3, queda una mas de la que viene de JD Edwards\n",
    "\n",
    "def week():\n",
    "    date = datetime.date.today()\n",
    "    year, week_num, day_of_week = date.isocalendar()\n",
    "    week = 'Week ' + str(week_num + 3)\n",
    "    return week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1612881132718,
     "user": {
      "displayName": "Juan Sanguineti",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-XfQmXcnLZFwq9qN4wdR7tl7YcAnxPy69Qje2Ug=s64",
      "userId": "06510006363156190395"
     },
     "user_tz": 180
    },
    "id": "cmtOfCgCKtvm"
   },
   "outputs": [],
   "source": [
    "#creates extruders output for SAC\n",
    "\n",
    "def extruders(schedule_bulk):\n",
    "    \n",
    "    #copy dataframe\n",
    "    extruders = schedule_bulk.copy()\n",
    "\n",
    "    #drop columns\n",
    "    extruders.drop(['week', \n",
    "                    'waste', \n",
    "                    'entity', \n",
    "                    'seed', \n",
    "                    ], axis=1, inplace = True)\n",
    "   \n",
    "    #rename colums\n",
    "    dictionary = {\"original due date\": \"Original Due Date\",\n",
    "                  \"ending hour\": \"Ending Hour\",\n",
    "                  \"starting hour\": \"Starting Hour\",\n",
    "                  \"extruder sequence\": \"Sequence\",\n",
    "                  \"run\":\"Run\", \n",
    "                  \"machine\" : \"WorkCenter\",\n",
    "                  \"starting\": \"Starting Date\",\n",
    "                  \"ending\": \"Ending Date\",\n",
    "                  \"prod planned\": \"Demand\", \n",
    "                  \"prod att\": \"Production\",\n",
    "                  \"shrinkage\": \"Shrinkage\",\n",
    "                  \"hours\": \"Hours\",\n",
    "                  \"fg assigned\": \"ItemRef\",\n",
    "                  \"last change over\":\"CO Time\",\n",
    "                  \"stuck time\" : \"Waiting Time\", \n",
    "                  \"bulk code\":\"ItemNumber\",\n",
    "                  \"purchase order\": \"Purchase Order\",\n",
    "                  \"sales order\":\"Sales Order\",\n",
    "                  \"work order\":\"Work Order Ref\",\n",
    "                  \"machine code\":\"WorkCenter Code\",\n",
    "                  \"facility\":\"Facility\",\n",
    "                  \"facility code\":\"Facility Code\"}\n",
    "    extruders.rename(columns= dictionary ,inplace=True)\n",
    "    \n",
    "    #fill nan with 0\n",
    "    extruders = extruders.fillna('0')\n",
    "\n",
    "    #insert process date, categorycode\n",
    "    extruders['Process Date'] = week()\n",
    "    extruders['CategoryCode'] = \"INT\"\n",
    "    \n",
    "    #change data types to int\n",
    "    extruders[\"Production\"] = extruders[\"Production\"].astype(float).astype(int)\n",
    "    extruders[\"Demand\"] = extruders[\"Demand\"].astype(float).astype(int)\n",
    "    \n",
    "    #from str to floats\n",
    "    extruders[\"Shrinkage\"] = extruders[\"Shrinkage\"].astype(float)\n",
    "    extruders[\"CO Time\"] = extruders[\"CO Time\"].astype(float)\n",
    "    extruders[\"Hours\"] = extruders[\"Hours\"].astype(float)\n",
    "    extruders[\"Waiting Time\"] = extruders[\"Waiting Time\"].astype(float)\n",
    "\n",
    "    #keep only dates of timestamp\n",
    "    extruders[\"Ending Date\"] = extruders[\"Ending Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "    extruders[\"Starting Date\"] = extruders[\"Starting Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "    extruders[\"Original Due Date\"] = extruders[\"Original Due Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "\n",
    "    #round \n",
    "    extruders = extruders.round(1)\n",
    "    \n",
    "    #replace missing with 0\n",
    "    extruders = extruders.replace('missing','0')\n",
    "    \n",
    "    #return dataframe\n",
    "    return extruders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inventory\n",
    "\n",
    "def inventory(bulk_inventory, extruders):\n",
    "    \n",
    "    #copy df\n",
    "    bulk_inventory_copy = bulk_inventory.copy()\n",
    "    \n",
    "    #rename columns\n",
    "    bulk_inventory_copy.rename(columns = {\n",
    "                                    'Component formula':'ItemNumber',\n",
    "                                    'Facility':'Facility Code',\n",
    "                                    'Quantity': 'Inventory'},\n",
    "                                    inplace = True)\n",
    "    \n",
    "    #process date y run\n",
    "    bulk_inventory_copy['Process Date'] = week()\n",
    "    bulk_inventory_copy['Run'] = extruders.loc[0,\"Run\"]\n",
    "    \n",
    "    bulk_inventory_copy['Facility'] = bulk_inventory_copy['Facility Code'].map({\n",
    "                                    '20001':'Bern', \n",
    "                                    '20005':'PC10', \n",
    "                                    '20006':'BAXTER SP',\n",
    "                                    '20005':'PC30'})\n",
    "    \n",
    "    #fill null values\n",
    "    bulk_inventory_copy = bulk_inventory_copy.fillna('0')\n",
    "\n",
    "    #data type\n",
    "    bulk_inventory_copy[\"Inventory\"] = bulk_inventory_copy[\"Inventory\"].astype(int)\n",
    "\n",
    "    \n",
    "    #category code\n",
    "    bulk_inventory_copy['CategoryCode'] = \"Inventory\"\n",
    "    \n",
    "    #replace missing with 0\n",
    "    bulk_inventory_copy = bulk_inventory_copy.replace('missing','0')\n",
    "\n",
    "    #return\n",
    "    return bulk_inventory_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates packline output for SAC\n",
    "\n",
    "def packlines(schedule_sku, extruders):\n",
    "        \n",
    "    #create a copy of the df\n",
    "    packlines = schedule_sku.copy()\n",
    "    \n",
    "\n",
    "    \n",
    "    # drop columns\n",
    "    packlines.drop(['order demand pounds',\n",
    "                    'production demand bags',\n",
    "                    'sku description',\n",
    "                    'inventory demand bags', \n",
    "                    'entity', \n",
    "                    'seed'\n",
    "                   ], axis=1, inplace = True)\n",
    "\n",
    "    # rename columns\n",
    "    dictionary = {\"ending hour\": \"Ending Hour\", \n",
    "                  \"starting hour\": \"Starting Hour\", \n",
    "                  \"starting date\": \"Starting Date\", \n",
    "                  \"ending date\": \"Ending Date\", \n",
    "                  \"original due date\": \"Original Due Date\",\n",
    "                  \"machines sequences\":\"Sequence\",\n",
    "                  \"run\": \"Run\", \n",
    "                  \"sku\": \"ItemNumber\", \n",
    "                  \"formula\": \"Diet\", \n",
    "                  \"machine\": \"WorkCenter\", \n",
    "                  \"production demand pounds\": \"Demand\", \n",
    "                  \"demand attained pounds\": \"Production\", \n",
    "                  \"order demand bags\": \"Bags Demand\", \n",
    "                  \"demand attained bags\": \"Bags Production\", \n",
    "                  \"hours\": \"Hours\",\n",
    "                  \"purchase order\": \"Purchase Order\",\n",
    "                  \"sales order\":\"Sales Order\",\n",
    "                  \"work order\":\"Work Order\",\n",
    "                  \"inventory demand pounds\": \"Inventory\",\n",
    "                  \"machine code\":\"WorkCenter Code\",\n",
    "                  \"facility\":\"Facility\",\n",
    "                  \"facility code\":\"Facility Code\"}\n",
    "                  \n",
    "    packlines.rename(columns = dictionary, inplace=True)\n",
    "    \n",
    "    #fill nan with 0\n",
    "    packlines = packlines.fillna('0')\n",
    "    \n",
    "    #change data type to int\n",
    "    packlines[\"Bags Production\"] = packlines[\"Bags Production\"].astype(float).astype(int)\n",
    "    packlines[\"Bags Demand\"] = packlines[\"Bags Demand\"].astype(float).astype(int)\n",
    "    packlines[\"Demand\"] = packlines[\"Demand\"].astype(float).astype(int)\n",
    "    packlines[\"Production\"] = packlines[\"Production\"].astype(float).astype(int)\n",
    "    packlines[\"Inventory\"] = packlines[\"Inventory\"].astype(float).astype(int)\n",
    "\n",
    "    #change to float\n",
    "    packlines[\"Hours\"] = packlines[\"Hours\"].astype(float)\n",
    "    \n",
    "    #round floats\n",
    "    packlines = packlines.round(1)\n",
    "    \n",
    "    #insert version, entity, process date, CategoryCode\n",
    "    packlines[\"Process Date\"] = week()\n",
    "    packlines['CategoryCode'] = \"FG\"\n",
    "\n",
    "    #keep only dates of timestamp\n",
    "    packlines[\"Starting Date\"] = packlines[\"Starting Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "    packlines[\"Ending Date\"] = packlines[\"Ending Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "    packlines[\"Original Due Date\"] = packlines[\"Original Due Date\"].str.split(\" \", n = 1, expand = True)[0]\n",
    "    \n",
    "    #replace missing with 0\n",
    "    packlines = packlines.replace('missing','0')\n",
    "    \n",
    "    #add run to packlines\n",
    "    packlines['Run'] = extruders.loc[0,\"Run\"]\n",
    "\n",
    "    #return dataframe\n",
    "    return packlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack for sac\n",
    "\n",
    "def unpacked(out_due_date_backlog, extruders):    \n",
    "    \n",
    "    #copy table\n",
    "    out_due_date_backlog_copy = out_due_date_backlog.copy()\n",
    "\n",
    "    #fill nan\n",
    "    out_due_date_backlog_copy.fillna('0', inplace=True)\n",
    "    \n",
    "    #insert version, date, week, run\n",
    "    out_due_date_backlog_copy['Process Date'] = week()\n",
    "    out_due_date_backlog_copy['run'] = extruders.loc[0,\"Run\"]\n",
    "    \n",
    "    #rename\n",
    "    out_due_date_backlog_copy.rename(columns={\n",
    "            \"run\": \"Run\", \n",
    "            'finished good': 'ItemNumber', \n",
    "            'due date':'Original Due Date',\n",
    "            'location': 'Facility',\n",
    "            'amount unpacked': 'Unpacked Amount',\n",
    "            'work order':'Work Order',\n",
    "            'purchase order':'Purchase Order'\n",
    "            }, inplace=True)\n",
    "\n",
    "    #category code for unpacked\n",
    "    out_due_date_backlog_copy['CategoryCode'] = \"Unpacked\"\n",
    "\n",
    "    #attribute name\n",
    "    out_due_date_backlog_copy.name = 'UNPACKED_SAC'\n",
    "    \n",
    "    #return unpack\n",
    "    return out_due_date_backlog_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create unified table of packlines and extrusion that will be uploaded to Hana\n",
    "\n",
    "def unified_sac(packlines, extruders, inventory, unpacked):\n",
    "    \n",
    "    #append one to the other\n",
    "    unified_table = extruders.append([packlines, \n",
    "                                     inventory, \n",
    "                                     unpacked], \n",
    "                                     ignore_index=True)\n",
    "\n",
    "    #fill nulls with 0\n",
    "    unified_table.fillna('0', inplace=True)\n",
    "    \n",
    "    #drop row if itemnumber is 0\n",
    "    unified_table = unified_table[unified_table.ItemNumber != '0']\n",
    "    \n",
    "    #replace missing with 0\n",
    "    unified_table = unified_table.replace('missing','0')\n",
    "    \n",
    "    #name attribute\n",
    "    unified_table.name = 'UNIFIED_SAC'\n",
    "\n",
    "    #return unified table\n",
    "    return unified_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create WO demand planning of SAGE to create the unassigned workorders\n",
    "\n",
    "def wo_demand(itemmaster,workorders):\n",
    "    \n",
    "    #create a copy\n",
    "    ItemMaster_copy = itemmaster.copy()\n",
    "    WorkOrders_copy = workorders.copy()\n",
    "    \n",
    "    #filter order status = 1 for Workorders\n",
    "    filter1 = WorkOrders_copy['OrderStatus'] == '1'\n",
    "    WorkOrders_copy = WorkOrders_copy[filter1]\n",
    "    \n",
    "    #rename columns\n",
    "    WorkOrders_copy.rename(columns= {'Facility':'Facility Code',\n",
    "                                     'Purchase_Order':'Purchase Order',\n",
    "                                     'WorkCenter':'WorkCenter Code'} ,inplace=True)\n",
    "\n",
    "    #merge dataframes\n",
    "    merge = WorkOrders_copy.merge(ItemMaster_copy[['ItemNumber', \n",
    "                                                   'CategoryCode',\n",
    "                                                   'ItemWeight'\n",
    "                                                   ]], \n",
    "                                                  on='ItemNumber',\n",
    "                                                  how = 'inner')\n",
    "    \n",
    "    \n",
    "    #filter based on two conditions\n",
    "    merge = merge[\n",
    "                  ((merge[\"CategoryCode\"]==\"INT\") & (merge[\"Operation\"]=='20')) | \n",
    "                  ((merge[\"CategoryCode\"]=='FG') & (merge[\"Operation\"]=='10'))\n",
    "                  ]\n",
    "    \n",
    "    \n",
    "    #planned - complete\n",
    "    merge['Demand'] = merge['PlannedQty'].astype(int) - merge['CompletedQty'].astype(int)\n",
    "\n",
    "    #multiply weight by plannedqty for FG\n",
    "    merge.loc[merge['CategoryCode'] == 'FG', 'Demand'] = merge['Demand'].astype(int) * merge['ItemWeight'].astype(float)\n",
    "        \n",
    "    #change data type\n",
    "    merge['Demand'] = merge['Demand'].astype(int)\n",
    "    \n",
    "    # 0 for negative numbers\n",
    "    merge.loc[merge['Demand'] < 0, 'Demand'] = 0 \n",
    "    \n",
    "    #fill null values with 0\n",
    "    merge.fillna('0', inplace=True)\n",
    "    \n",
    "    #run and process date\n",
    "    merge['Run'] = extruders.loc[0,\"Run\"]\n",
    "    merge['Process Date'] = week()\n",
    "    \n",
    "    #keep only dates\n",
    "    merge[\"PlannedStart\"] = merge[\"PlannedStart\"].str.split(\"T\", n = 1, expand = True)[0]\n",
    "    merge[\"PlannedEnd\"] = merge[\"PlannedEnd\"].str.split(\"T\", n = 1, expand = True)[0]\n",
    "    \n",
    "    #drop columns\n",
    "    merge.drop(['OrderStatus', 'Operation', 'ItemWeight', 'PlannedQty', 'CompletedQty'], axis=1, inplace = True)\n",
    "    \n",
    "    #round decimals\n",
    "    merge = merge.round(1)\n",
    "    \n",
    "    #name \n",
    "    merge.name = \"WO_DEMAND\"\n",
    "    \n",
    "    #return dataframe\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped itemnumbers per uninterrupted sequence and workcenter\n",
    "\n",
    "def group_extruders(extruders, inventory):\n",
    "\n",
    "    #create copy\n",
    "    group_extruders = extruders.copy()\n",
    "    inventory_copy = inventory.copy()\n",
    "\n",
    "    #inventory merge\n",
    "    group_extruders = group_extruders.merge(inventory_copy[['ItemNumber', 'Inventory']], \n",
    "                            on='ItemNumber', \n",
    "                            how = 'left')\n",
    "    \n",
    "    #drop columns\n",
    "    group_extruders.drop(['Shrinkage', 'CO Time','Waiting Time'], axis=1, inplace = True)\n",
    "\n",
    "    #change sequence data type and sort the df by WC and sequence\n",
    "    group_extruders['Sequence'] = group_extruders['Sequence'].astype(int)\n",
    "    group_extruders = group_extruders.sort_values(['WorkCenter','Sequence'])\n",
    "    group_extruders['Sequence'] = group_extruders['Sequence'].astype(str)\n",
    "\n",
    "    #keep first colum of group to keep start date\n",
    "    first = group_extruders[['ItemNumber',\n",
    "                             'Sequence', \n",
    "                             'Purchase Order',\n",
    "                             'Starting Date', \n",
    "                             'Starting Hour',\n",
    "                             'Run',\n",
    "                             'Process Date',\n",
    "                             'WorkCenter',\n",
    "                             'Facility',\n",
    "                             'WorkCenter Code',\n",
    "                             'Facility Code',\n",
    "                             'Inventory',\n",
    "                             'Work Order Ref'\n",
    "                            ]].groupby(by=[(\n",
    "                            group_extruders.ItemNumber!=group_extruders.ItemNumber.shift()).cumsum()\n",
    "                            ], \n",
    "                            as_index=False).nth([0]).reset_index(drop=True)\n",
    "\n",
    "    #keep last row of group to keep end date\n",
    "    last = group_extruders[['ItemNumber', \n",
    "                            'Ending Date', \n",
    "                            'Ending Hour'\n",
    "                           ]].groupby(by=[(\n",
    "                            group_extruders.ItemNumber!=group_extruders.ItemNumber.shift()).cumsum()\n",
    "                            ], as_index=False).nth([-1]).reset_index(drop=True)\n",
    "\n",
    "    #merge first and last rows of group to have the first and last date on same row\n",
    "    firstlast = pd.merge(first, last, \n",
    "                             how='inner', \n",
    "                             on='ItemNumber', \n",
    "                             left_index=True, \n",
    "                             right_index=True)\n",
    "    \n",
    "    #drop inventory to not sum\n",
    "    group_extruders.drop('Inventory', inplace = True, axis = 1)\n",
    "\n",
    "    #sum all measures of group\n",
    "    suma = group_extruders.groupby(by=[(\n",
    "                            group_extruders.ItemNumber!=group_extruders.ItemNumber.shift()).cumsum(),\n",
    "                            'ItemNumber'\n",
    "                            ], as_index=False).sum()\n",
    "\n",
    "    #merge measures with last and first rows\n",
    "    merge = pd.merge(firstlast, suma, \n",
    "                             how='inner', \n",
    "                             on='ItemNumber', \n",
    "                             left_index=True, \n",
    "                             right_index=True)\n",
    "    \n",
    "    #fill null with 0\n",
    "    merge.fillna('0', inplace=True)\n",
    "\n",
    "    #round decimals\n",
    "    merge = merge.round(1)\n",
    "    \n",
    "    #return df\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigned wo to group extrusion\n",
    "\n",
    "def assigned_wo(group_extruders, wo_demand):\n",
    "    \n",
    "    #create a copu\n",
    "    group_extruders_copy = group_extruders.copy()\n",
    "    wo_bulk_copy = wo_demand.copy()\n",
    "    \n",
    "    #filter demand per categorycode = INT\n",
    "    filter2 = wo_bulk_copy['CategoryCode'] == 'INT'\n",
    "    wo_bulk_copy = wo_bulk_copy[filter2]\n",
    "\n",
    "    #sort key values\n",
    "    group_extruders_copy = group_extruders_copy.sort_values('Production')\n",
    "    wo_bulk_copy = wo_bulk_copy.sort_values('Demand')\n",
    "    \n",
    "    #drop demand from grouped\n",
    "    group_extruders_copy.drop(['Demand'], inplace = True, axis = 1)\n",
    "    \n",
    "    #merge as of\n",
    "    merge = pd.merge_asof(group_extruders_copy, \n",
    "                          wo_bulk_copy[[\n",
    "                            'Demand', \n",
    "                            'ItemNumber', \n",
    "                            'WorkOrderNumber']], \n",
    "                              left_on =\"Production\", \n",
    "                              right_on = 'Demand', \n",
    "                              direction = 'nearest',\n",
    "                              by = 'ItemNumber')\n",
    "    \n",
    "    #create boolean threshold on dummy\n",
    "    merge['dummy'] = (merge['Production'] / merge['Demand']) * 100\n",
    "    merge['dummy'] = merge['dummy'].between(95, 105)\n",
    "    \n",
    "    #conditional threshold to set values\n",
    "    merge['Close/Distant Assigned'] = merge['dummy'].map({True: 'Close', False: 'Distant'})\n",
    "        \n",
    "    #drop dummy\n",
    "    merge.drop(['dummy'], axis=1, inplace = True)\n",
    "    \n",
    "    #rename\n",
    "    merge.rename({'WorkOrderNumber':'WorkOrder Assigned'}, inplace = True, axis = 1)\n",
    "    \n",
    "    #create time column\n",
    "    merge['Starting Time'] = merge['Starting Date'] + ' ' + merge['Starting Hour']\n",
    "    merge['Ending Time'] = merge['Ending Date'] + ' ' + merge['Ending Hour']\n",
    "    \n",
    "    #fill nulls with 0\n",
    "    merge.fillna('0', inplace=True)\n",
    "    \n",
    "    #drop if itemnumber ==0\n",
    "    merge = merge[merge.ItemNumber != '0']\n",
    "\n",
    "    #attribute name\n",
    "    merge.name = \"GROUPE_EXTRUDERS_ASSIGNED_SAC\"\n",
    "    \n",
    "    #return merge\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for unified df \n",
    "\n",
    "extruders = extruders(schedule_bulk)\n",
    "packlines = packlines(schedule_sku, extruders)\n",
    "unpacked = unpacked(out_due_date_backlog, extruders)\n",
    "inventory = inventory(bulk_inventory, extruders)\n",
    "wo_demand = wo_demand(itemmaster,workorders)\n",
    "unified_sac = unified_sac(packlines, extruders, inventory, unpacked)\n",
    "\n",
    "#variables for assigned wo\n",
    "\n",
    "group_extruders = group_extruders(extruders, inventory)\n",
    "assigned_wo = assigned_wo(group_extruders, wo_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sube la lista de tablas de SACA a Hana y pisa segun week y run\n",
    "\n",
    "def upload_output_to_hana():\n",
    "    \n",
    "    #lista_tablas_para_SAC = [wo_demand]\n",
    "    #lista_tablas_para_SAC = [wo_bulk, assigned_wo, unified_sac]\n",
    "    lista_tablas_para_SAC = [assigned_wo, unified_sac, wo_demand]\n",
    "\n",
    "    #coneccion a variable\n",
    "    connection = connectToHANA()\n",
    "    \n",
    "    #itera sobre las tablas, pisa segun run y process date. Si no funciona, dale error\n",
    "    for table in lista_tablas_para_SAC:\n",
    "        try:\n",
    "\n",
    "            #variables de run y process date\n",
    "            Run = table.loc[1,\"Run\"]\n",
    "            Process_Date = table.loc[1,\"Process Date\"]\n",
    "\n",
    "            #execute sql to delete rows on database based on run and process date\n",
    "            connection.execute(f\"\"\"DELETE FROM \"SAC_OUTPUT\".{table.name} WHERE \"Process Date\" = '{Process_Date}' and \"Run\" = '{Run}'\"\"\")\n",
    "            print('Values deleted succesfully')\n",
    "\n",
    "            #append dataframe to the table\n",
    "            table.to_sql(table.name.lower(), schema='SAC_OUTPUT', con=connection, if_exists='append', index=False)\n",
    "            print(table.name + ' uploaded succesfully')\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            #print problems\n",
    "            print(table.name +' failed to upload! ' + str(e))\n",
    "           \n",
    "    #close hana connection\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values deleted succesfully\n",
      "GROUPE_EXTRUDERS_ASSIGNED_SAC uploaded succesfully\n",
      "Values deleted succesfully\n",
      "UNIFIED_SAC uploaded succesfully\n",
      "Values deleted succesfully\n",
      "WO_DEMAND uploaded succesfully\n"
     ]
    }
   ],
   "source": [
    "upload_output_to_hana()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyRr90otIVsyMnSYvxrxzq",
   "name": "Save output to hana.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
