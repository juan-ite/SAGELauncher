{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "executionInfo": {
     "elapsed": 886,
     "status": "ok",
     "timestamp": 1612881125722,
     "user": {
      "displayName": "Juan Sanguineti",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-XfQmXcnLZFwq9qN4wdR7tl7YcAnxPy69Qje2Ug=s64",
      "userId": "06510006363156190395"
     },
     "user_tz": 180
    },
    "id": "zyvGUVHeJdY1"
   },
   "outputs": [],
   "source": [
    "import sqlalchemy_hana\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import hdbcli\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week():\n",
    "    #toma la week de hoy y le suma 3, queda una mas de la que viene de JD Edwards\n",
    "    \n",
    "    date = datetime.date.today()\n",
    "    year, week_num, day_of_week = date.isocalendar()\n",
    "    week = 'Week ' + str(week_num + 3)\n",
    "    return week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1612881132718,
     "user": {
      "displayName": "Juan Sanguineti",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-XfQmXcnLZFwq9qN4wdR7tl7YcAnxPy69Qje2Ug=s64",
      "userId": "06510006363156190395"
     },
     "user_tz": 180
    },
    "id": "cmtOfCgCKtvm"
   },
   "outputs": [],
   "source": [
    "def extruders_output():\n",
    "    #create extruders_output\n",
    "    \n",
    "    extruders = pd.read_sql('SELECT * FROM \"ANYLOGIC\".\"SCHEDULE_BULK\"', con=connectToHANA())\n",
    "    \n",
    "    extruders[\"starting\"] = pd.to_datetime(extruders['starting'], errors='coerce')\n",
    "    extruders[\"ending\"] = pd.to_datetime(extruders['ending'], errors='coerce')\n",
    "    extruders[\"original due date\"] = pd.to_datetime(extruders['original due date'], errors='coerce')\n",
    "\n",
    "    extruders[\"starting time\"] = np.nan\n",
    "    extruders[\"ending time\"] = np.nan\n",
    "\n",
    "    lst = [\"bulk code\",\"fg assigned\" ,\"machine\", \"starting\", \"starting hour\", \"starting time\", \"ending\", \n",
    "        \"ending hour\", \"ending time\", \"extruder sequence\", \"purchase order\", \n",
    "        \"sales order\",\"original due date\",\"work order\",'run', \"prod planned\", \"prod att\", \n",
    "        \"shrinkage\", \"hours\",\"last change over\",\"stuck time\"]\n",
    "    extruders = extruders[lst].copy()\n",
    "\n",
    "    #rename columns\n",
    "    dict = {'run':'Run',\"machine\": \"work center\", \"starting\": \"starting date\", \"ending\": \"ending date\",\n",
    "          \"prod planned\": \"P_Planned\", \"prod att\": \"P_Att\", \"shrinkage\": \"Shrink_P\", \"hours\": \"Hours\",\n",
    "          \"fg assigned\": \"Finished Good\", \"last change over\":\"Last_CO\", \"stuck time\" : \"S_Time\"}\n",
    "    extruders.rename(columns= dict ,inplace=True)\n",
    "    \n",
    "    #setting date and time\n",
    "    \n",
    "    \n",
    "    extruders[\"starting time\"] = extruders[\"starting date\"]\n",
    "    extruders[\"ending time\"] = extruders[\"ending date\"]\n",
    "    extruders[\"starting date\"] = pd.to_datetime(extruders[\"starting date\"].dt.strftime(\"%Y/%m/%d\")).astype(str)\n",
    "    extruders[\"ending date\"]  = pd.to_datetime(extruders[\"ending date\"].dt.strftime(\"%Y/%m/%d\")).astype(str)\n",
    "    extruders[\"original due date\"] = pd.to_datetime(extruders[\"original due date\"].dt.strftime(\"%Y/%m/%d\"))\n",
    "    extruders['original due date'] = extruders['original due date'].astype(str).str.split().str[0]\n",
    "    \n",
    "\n",
    "    extruders['work order'].fillna(\"missing\", inplace = True)\n",
    "    extruders['work order'].replace(\"missing\",\"0\", inplace = True)\n",
    "    extruders[\"work order\"] = [str(order).split(\".\")[0] for order in extruders[\"work order\"]]\n",
    "    \n",
    "    extruders['sales order'].replace('missing', 0, inplace=True)\n",
    "    \n",
    "    #insert version and entity\n",
    "    extruders.insert(2, 'Version', 'SIM')\n",
    "    extruders.insert(3, 'Entity', 'CJ Foods')\n",
    "\n",
    "    #add process date\n",
    "    extruders.insert(1, 'Process Date', week()) \n",
    "    \n",
    "    #attribute name\n",
    "    extruders.name = 'EXTRUDERS_SAC'\n",
    "\n",
    "    return extruders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_extrusion():\n",
    "    #pre grouped extrusion output\n",
    "    # Takes Extrusion output and groups the bulks if they are next to each other in the time frame\n",
    "    model_output = pd.read_sql('SELECT * FROM \"ANYLOGIC\".\"SCHEDULE_BULK\"', con=connectToHANA())\n",
    "    model_output[\"starting\"] = pd.to_datetime(model_output['starting'], errors='coerce')\n",
    "    model_output[\"ending\"] = pd.to_datetime(model_output['ending'], errors='coerce')\n",
    "    j = 0\n",
    "    sequence = 0\n",
    "    output_file = model_output.copy() #:: create copy of original        \n",
    "    output_file = output_file[[\"seed\",'machine','machine code', 'bulk code',\n",
    "                               'starting','ending','hours','prod planned','prod att', 'run']]\n",
    "\n",
    "    output_file = output_file.sort_values(by = ['machine', 'machine code', 'bulk code', 'starting', 'ending'],\n",
    "                                                 ascending = [True, True, True, True, True])\n",
    "    output_file.reset_index(inplace=True)\n",
    "    \n",
    "    for idx, row in output_file.iterrows():\n",
    "        if idx>0 :\n",
    "            current_start_date = output_file.loc[idx,'starting']\n",
    "            current_end_date = output_file.loc[idx,'ending']\n",
    "            current_bulk = output_file.loc[idx,'bulk code']\n",
    "            current_machine_code = output_file.loc[idx,'machine code']\n",
    "            j = idx-1;\n",
    "            previous_start_date = output_file.loc[j,'starting']\n",
    "            previous_end_date = output_file.loc[j,'ending']\n",
    "            previous_bulk = output_file.loc[j,'bulk code']\n",
    "            previous_machine_code = output_file.loc[j,'machine code']\n",
    "\n",
    "            if current_bulk == previous_bulk and current_machine_code == previous_machine_code:\n",
    "                if(current_start_date == previous_end_date):   \n",
    "                    output_file.loc[idx, 'sequence'] = sequence\n",
    "                    output_file.loc[j, 'sequence'] = sequence   \n",
    "                else:\n",
    "                    output_file.loc[idx, 'sequence'] = -1\n",
    "                    sequence += 1\n",
    "            else:\n",
    "                output_file.loc[idx, 'sequence'] = -2\n",
    "\n",
    "        else:\n",
    "            output_file.loc[idx, 'sequence'] = -3\n",
    "                \n",
    "                \n",
    "    \n",
    "    # Gets the correct start date and end date for the particular bulk-sequence group after the grouping\n",
    "    missing = {}\n",
    "    for idx, row in output_file.iterrows():\n",
    "        machine_code = row['machine code']\n",
    "        start_date = row['starting']\n",
    "        end_date = row['ending']\n",
    "        bulk = row['bulk code']\n",
    "        sequence = row['sequence']\n",
    "        if machine_code not in missing.keys():\n",
    "            missing[machine_code] = {bulk : {}}\n",
    "            missing[machine_code][bulk][sequence] =  (start_date,end_date)         \n",
    "        else:\n",
    "            if bulk not in missing[machine_code]:                \n",
    "                missing[machine_code].update({bulk : {}})\n",
    "                if sequence not in missing[machine_code][bulk]:\n",
    "                    missing[machine_code][bulk][sequence] =  (start_date,end_date)            \n",
    "            else:\n",
    "                if sequence in missing[machine_code][bulk]:\n",
    "                    last_start = missing[machine_code][bulk][sequence][0]\n",
    "                    last_end = missing[machine_code][bulk][sequence][1]\n",
    "                    if(start_date<last_start) : \n",
    "                        last_start = start_date\n",
    "                    if(end_date>last_end) : \n",
    "                        last_end = end_date  \n",
    "                else:\n",
    "                    last_start = start_date\n",
    "                    last_end = end_date \n",
    "                \n",
    "                missing[machine_code][bulk][sequence] = (last_start,last_end)\n",
    "    \n",
    "    data = missing\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    output_file = output_file.groupby(by = ['machine',\n",
    "                                            'machine code', \n",
    "                                            'bulk code', \n",
    "                                            'sequence'], as_index = False).sum()   \n",
    "    \n",
    "    for machine, dic in data.items():\n",
    "        for bulk, dic_2 in dic.items():\n",
    "            for sequence, dates in dic_2.items():\n",
    "                output_file.loc[(output_file['machine code'] == machine) & \n",
    "                                (output_file['bulk code'] == bulk) & \n",
    "                                (output_file['sequence'] == sequence), 'start_date'] = dates[0]\n",
    "                \n",
    "                \n",
    "                output_file.loc[(output_file['machine code'] == machine) & \n",
    "                                (output_file['bulk code'] == bulk) &\n",
    "                                (output_file['sequence'] == sequence), 'ending_date'] = dates[1]    \n",
    "    \n",
    "    output_file = output_file[['seed','machine', 'machine code', 'bulk code','start_date',\n",
    "                               'ending_date','hours','prod planned','prod att']]\n",
    "    \n",
    "\n",
    "    output_file['run'] = model_output.loc[1,'run']\n",
    "    output_file.sort_values(by = [\"start_date\"], ascending = [True], inplace = True)\n",
    "    output_file.reset_index(drop = True, inplace = True)\n",
    "    output_file['sequence'] = [i for i in range(len(output_file))]\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_extrusion_output():\n",
    "    #creates grouped extrusion output\n",
    "    ItemMaster = pd.read_sql('SELECT * FROM \"SAGE\".\"ITEMMASTER\"', con=connectToHANA())\n",
    "    bulk_demand = pd.read_sql('SELECT * FROM \"SAGE\".\"WORKORDERS\"', con=connectToHANA())\n",
    "\n",
    "    bulk_demand.rename(columns={'PlannedEnd': 'Due_Date','ItemNumber': 'Bulk_Code', 'PlannedQty': 'P_Att', 'WorkOrderNumber':'Work_Order'}, inplace=True)\n",
    "    bulk_demand['Due_Date'] = pd.to_datetime(bulk_demand['Due_Date'])\n",
    "    bulk_demand.sort_values('Due_Date', ascending=True)\n",
    "\n",
    "    bulk_demand_og = bulk_demand.copy()\n",
    "\n",
    "    filter1 = bulk_demand['OrderStatus'] == 1\n",
    "    bulk_demand = bulk_demand[filter1]\n",
    "\n",
    "    bulk_demand = bulk_demand.merge(ItemMaster[['ItemNumber', 'CategoryCode']], left_on = 'Bulk_Code', right_on='ItemNumber', how = 'left')\n",
    "\n",
    "    filter2 = bulk_demand['CategoryCode'] == 'INT'\n",
    "    bulk_demand = bulk_demand[filter2]\n",
    "\n",
    "    wo_po = bulk_demand[['Bulk_Code', 'Work_Order','P_Att', 'Purchase_Order']].drop_duplicates(subset=['Bulk_Code', 'P_Att'])\n",
    "    wo_po['P_Att'] = wo_po['P_Att'].apply(round).apply(int)\n",
    "\n",
    "\n",
    "    # toma de la funcion anterior el output\n",
    "    df = group_extrusion()\n",
    "    del df['machine']\n",
    "\n",
    "    df.rename(columns={\"run\": \"Run\",'machine code': 'Work_Center', 'bulk code': 'Bulk_Code', 'hours': 'Hours', 'prod att': 'P_Att', \n",
    "                 'prod planned': 'P_Planned', 'sequence': 'Machine_Sequence',\n",
    "                 'start_date': 'start_time','ending_date': 'ending_time'},inplace=True)\n",
    "\n",
    "    final_df = df[['Work_Center', 'Bulk_Code', 'start_time', 'ending_time', 'Machine_Sequence','Hours', 'Run', 'P_Att','P_Planned']].copy()\n",
    "\n",
    "\n",
    "    final_df['P_Att'] = final_df['P_Att'].apply(round).apply(int)\n",
    "\n",
    "    final_df = final_df.merge(wo_po, how='left', on=['Bulk_Code', 'P_Att'])\n",
    "    final_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Fix WOs in cases where the demand is the same for a bulk that appears many times, with differents WOs. Returns: Fixed Grouped Output DF\n",
    "    # k will be the index for the wos list\n",
    "    k = 0\n",
    "    for ix, row in bulk_demand_og.iterrows():\n",
    "        bulk = row['Bulk_Code']\n",
    "        dem = row['P_Att']\n",
    "        wo = row['Work_Order']\n",
    "        if bulk in final_df['Bulk_Code'].unique():\n",
    "            out_bulk = final_df.loc[final_df['Bulk_Code'] == bulk, 'Bulk_Code'].values[0]\n",
    "            out_prod = final_df.loc[final_df['Bulk_Code'] == bulk, 'P_Att'].values[0]\n",
    "\n",
    "            if (bulk == out_bulk) and (dem == out_prod):\n",
    "                # List of all WOs for a bulk, when the demand is the same as the attained production\n",
    "                wos = bulk_demand_og.loc[(bulk_demand_og['Bulk_Code'] == bulk), 'Work_Order'].tolist()\n",
    "                for i in final_df.loc[(final_df['Bulk_Code'] == bulk) & (final_df['P_Att'] == dem)].index:\n",
    "                    # need to loop through index to fix it in place\n",
    "                    final_df.loc[i, 'Work_Order'] = wos[k]\n",
    "                    k += 1\n",
    "            k = 0\n",
    "\n",
    "\n",
    "    #\n",
    "    final_df['start_time'] = final_df['start_time'].astype(str)\n",
    "    final_df['ending_time'] = final_df['ending_time'].astype(str)\n",
    "    final_df['start_date'] = pd.to_datetime(final_df['start_time'].str.split().str[0]).dt.date\n",
    "    final_df['ending_date'] = pd.to_datetime(final_df['ending_time'].str.split().str[0]).dt.date\n",
    "    final_df['start_hour'] = pd.to_datetime(final_df['start_time'].str.split().str[1]).dt.time\n",
    "    final_df['ending_hour'] = pd.to_datetime(final_df['ending_time'].str.split().str[1]).dt.time\n",
    "\n",
    "    final_df['start_time'] = pd.to_datetime(final_df['start_time'].astype(str))\n",
    "    final_df['ending_time'] = pd.to_datetime(final_df['ending_time'].astype(str))\n",
    "\n",
    "    final_df.insert(1, 'Process Date', week()) \n",
    "\n",
    "\n",
    "    final_df['Entity'] = 'CJ Foods'\n",
    "    final_df[\"Version\"] = \"SIM\"\n",
    "\n",
    "\n",
    "    final_df = final_df[['Work_Center', 'Bulk_Code', 'start_date', 'start_hour', 'start_time', 'ending_date', 'ending_hour',\n",
    "                         'ending_time', 'Machine_Sequence', 'Run', 'Process Date', 'Entity', 'Version', \n",
    "                         'Purchase_Order', 'Work_Order', 'P_Att','Hours','P_Planned']].copy()\n",
    "\n",
    "\n",
    "    final_df['Work_Order'].fillna('0', inplace=True)\n",
    "    final_df['Purchase_Order'].fillna('0', inplace=True)\n",
    "\n",
    "    final_df['Work_Order'] = [str(order).split(\".\")[0] for order in final_df['Work_Order']]\n",
    "    final_df['Work_Center'] = [str(order).split(\".\")[0] for order in final_df['Work_Center']]\n",
    "    final_df['Bulk_Code'] = [str(order).split(\".\")[0] for order in final_df['Bulk_Code']]\n",
    "\n",
    "    final_df.name = 'GROUPED_EXTRUSION_SAC'\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def packlines_output():\n",
    "    #PACKLINES output\n",
    "    \n",
    "    packlines = pd.read_sql('SELECT * FROM \"ANYLOGIC\".\"SCHEDULE_SKU\"', con=connectToHANA())\n",
    "    extruders = pd.read_sql('SELECT * FROM \"ANYLOGIC\".\"SCHEDULE_BULK\"', con=connectToHANA())\n",
    "    packlines['run'] = extruders.loc[0,\"run\"]\n",
    "    \n",
    "    packlines[\"starting time\"] = np.nan\n",
    "    packlines[\"ending time\"] = np.nan\n",
    "    packlines = packlines[[\"sku\", \"formula\", \"machine\", \"starting date\", \"starting hour\", \"starting time\",\n",
    "                           \"ending date\", \"ending hour\", \"ending time\", \"machines sequences\", \"purchase order\", \n",
    "                           \"sales order\",\"original due date\",\"work order\",\"entity\", 'run', \"order demand pounds\",\n",
    "                           \"production demand pounds\", \"demand attained pounds\", \"order demand bags\",\n",
    "                           \"production demand bags\",\"demand attained bags\", \"hours\", \"seed\"]].copy()\n",
    "\n",
    "    packlines.rename(columns={\"entity\": \"Entity\", \"run\": \"Run\", \"sku\": \"item code\", \"formula\": \"diet\", \"machine\": \"work center\", \"order demand pounds\": \n",
    "                              \"Order_Dem\", \"production demand pounds\": \"Prod_Dem\", \"demand attained pounds\": \"Att_Dem\", \n",
    "                              \"order demand bags\": \"Order_Bags\", \"production demand bags\": \"Prod_Bags\",\n",
    "                              \"demand attained bags\": \"Att_Bags\", \"hours\": \"Hours\"}, inplace=True)\n",
    "\n",
    "    packlines[\"starting time\"] = packlines[\"starting date\"]\n",
    "    packlines[\"ending time\"]   = packlines[\"ending date\"]\n",
    "    packlines['work order'].fillna(\"0\", inplace = True)\n",
    "    packlines['work order'].replace(\"missing\",\"0\", inplace = True)\n",
    "    packlines[\"work order\"]    = [str(order).split(\".\")[0] for order in packlines[\"work order\"]]\n",
    "    packlines[\"starting date\"] = pd.to_datetime(packlines[\"starting date\"].dt.strftime(\"%Y/%m/%d\")).astype(str)\n",
    "    packlines[\"ending date\"]   = pd.to_datetime(packlines[\"ending date\"].dt.strftime(\"%Y/%m/%d\")).astype(str)\n",
    "    packlines[\"original due date\"] = pd.to_datetime(packlines[\"original due date\"].dt.strftime(\"%Y/%m/%d\"))\n",
    "\n",
    "    packlines_input = packlines\n",
    "\n",
    "    packlines_input[\"Version\"] = \"SIM\"\n",
    "\n",
    "    packlines_input.insert(1, 'Process Date', week()) \n",
    "\n",
    "    packlines_input['original due date'] = packlines_input['original due date'].astype(str).str.split().str[0]\n",
    "\n",
    "    packlines_input['sales order'].replace('missing', 0, inplace=True)\n",
    "    packlines_input['work order'].replace('missing', 0, inplace=True)\n",
    "\n",
    "    packlines_input.name = 'PACKLINES_SAC'\n",
    "    \n",
    "    return packlines_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpacked_output():    \n",
    "    #UNPACKED output\n",
    "\n",
    "    df = pd.read_sql('SELECT * FROM \"ANYLOGIC\".\"OUT_DUE_DATE_BACKLOG\"', con=connectToHANA())\n",
    "    extruders = pd.read_sql('SELECT * FROM \"ANYLOGIC\".\"SCHEDULE_BULK\"', con=connectToHANA())\n",
    "\n",
    "    df = df[['finished good', 'due date', 'location', 'work order', 'purchase order', 'amount unpacked']]\n",
    "    df['work order'] = df['work order'].apply(str)\n",
    "    df['work order'] = [wo[0] for wo in df['work order'].str.split('.')]\n",
    "    df['purchase order'].fillna('#', inplace=True)\n",
    "    df['Version'] = 'SIM'\n",
    "    df['Process Date'] = week()\n",
    "    \n",
    "    df['run'] = extruders.loc[0,\"run\"]\n",
    "    df['Entity'] = 'CJ Foods'\n",
    "    df = df[['finished good', 'due date', 'location', 'work order', 'purchase order', 'Version', 'Process Date', 'run', 'Entity',\n",
    "            'amount unpacked']]\n",
    "    df.rename(columns={\"run\": \"Run\", 'finished good': 'Item_Code', 'due date':'Due Date'}, inplace=True)\n",
    "    df = df[df['location'] != 'None']\n",
    "    df.name = 'UNPACKED_SAC'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_packing_output():\n",
    "    #GROUP PACKLINES\n",
    "    \n",
    "    model_output = pd.read_sql('SELECT * FROM \"ANYLOGIC\".\"SCHEDULE_SKU\"', con=connectToHANA())\n",
    "    extruders = pd.read_sql('SELECT * FROM \"ANYLOGIC\".\"SCHEDULE_BULK\"', con=connectToHANA())\n",
    "    \n",
    "    # Takes Packlines output and groups the FG if they are next to each other in the time frame\n",
    "    j = 0\n",
    "    sequence = 0\n",
    "    sku_sheet = model_output.copy() #:: create copy of original\n",
    "    mask = sku_sheet.sku.str.contains(\"_diet_\")\n",
    "    sku_sheet_groupable = sku_sheet.loc[mask]\n",
    "        \n",
    "    sku_sheet_not_groupable = sku_sheet.loc[~mask]\n",
    "    \n",
    "    sku_sheet_not_groupable.rename(columns = {\"starting date\": \"start_date\", \n",
    "                                              \"ending date\" : \"ending_date\"}, inplace = True)\n",
    "\n",
    "    sku_sheet_groupable = sku_sheet_groupable[[\"seed\",'machine','machine code', 'sku',\n",
    "                                               'starting date','ending date','hours','order demand pounds',\n",
    "                                               'demand attained pounds', 'production demand pounds', 'order demand bags',\n",
    "                                               'production demand bags', 'demand attained bags',\n",
    "                                               'sku description', 'formula', 'inventory demand pounds', \n",
    "                                               'inventory demand bags', 'purchase order', 'sales order', 'original due date', \n",
    "                                               'entity', 'work order']]\n",
    "    \n",
    "    \n",
    "\n",
    "    sku_sheet_groupable['sku'] = sku_sheet_groupable['sku'].map(lambda x : x.split(\"_diet_\")[0])\n",
    "\n",
    "    sku_sheet_groupable = sku_sheet_groupable.sort_values(by = ['machine', 'machine code', 'sku', 'starting date', 'ending date'],\n",
    "                                                            ascending = [True, True, True, True, True])\n",
    "    sku_sheet_groupable.reset_index(inplace=True)\n",
    "\n",
    "    sku_sheet_groupable[\"sequence\"] = \"\"\n",
    "\n",
    "    for idx, row in sku_sheet_groupable.iterrows():\n",
    "        if idx>0 :\n",
    "            current_start_date = sku_sheet_groupable.loc[idx,'starting date']\n",
    "            current_end_date = sku_sheet_groupable.loc[idx,'ending date']\n",
    "            current_bulk = sku_sheet_groupable.loc[idx,'sku']\n",
    "            current_machine_code = sku_sheet_groupable.loc[idx,'machine code']\n",
    "            j = idx-1;\n",
    "            previous_start_date = sku_sheet_groupable.loc[j,'starting date']\n",
    "            previous_end_date = sku_sheet_groupable.loc[j,'ending date']\n",
    "            previous_bulk = sku_sheet_groupable.loc[j,'sku']\n",
    "            previous_machine_code = sku_sheet_groupable.loc[j,'machine code']\n",
    "\n",
    "            if current_bulk == previous_bulk and current_machine_code == previous_machine_code:\n",
    "                if(current_start_date == previous_end_date):   \n",
    "                    sku_sheet_groupable.loc[idx, 'sequence'] = sequence\n",
    "                    sku_sheet_groupable.loc[j, 'sequence'] = sequence\n",
    "\n",
    "                else:\n",
    "                    sku_sheet_groupable.loc[idx, 'sequence'] = -1\n",
    "                    sequence += 1\n",
    "\n",
    "            else:\n",
    "                sku_sheet_groupable.loc[idx, 'sequence'] = -2\n",
    "        else:\n",
    "            sku_sheet_groupable.loc[idx, 'sequence'] = -3\n",
    "    \n",
    "    \n",
    "    # Gets the correct start date and end date for the particular FG-sequence group after the grouping\n",
    "    missing = {}\n",
    "    for idx, row in sku_sheet_groupable.iterrows():\n",
    "        machine_code = row['machine code']\n",
    "        start_date = row['starting date']\n",
    "        end_date = row['ending date']\n",
    "        bulk = row['sku']\n",
    "        sequence = row['sequence']\n",
    "        if machine_code not in missing.keys():\n",
    "            missing[machine_code] = {bulk : {}}\n",
    "            missing[machine_code][bulk][sequence] =  (start_date,end_date)         \n",
    "        else:\n",
    "            if bulk not in missing[machine_code]:                \n",
    "                missing[machine_code].update({bulk : {}})\n",
    "                if sequence not in missing[machine_code][bulk]:\n",
    "                    missing[machine_code][bulk][sequence] =  (start_date,end_date)            \n",
    "            else:\n",
    "                if sequence in missing[machine_code][bulk]:\n",
    "                    last_start = missing[machine_code][bulk][sequence][0]\n",
    "                    last_end = missing[machine_code][bulk][sequence][1]\n",
    "                    if(start_date<last_start) : \n",
    "                        last_start = start_date\n",
    "                    if(end_date>last_end) : \n",
    "                        last_end = end_date  \n",
    "                else:\n",
    "                    last_start = start_date\n",
    "                    last_end = end_date \n",
    "\n",
    "                missing[machine_code][bulk][sequence] = (last_start, last_end)\n",
    "    \n",
    "    data = missing\n",
    "    \n",
    "    #continue\n",
    "    sku_sheet_groupable = sku_sheet_groupable.groupby(by = ['machine',\n",
    "                                            'machine code', \n",
    "                                            'sku' ,            \n",
    "                                            'sequence',\n",
    "                                            'formula', \n",
    "                                            'purchase order', \n",
    "                                            'sku description', \n",
    "                                            'sales order', \n",
    "                                            'entity', \n",
    "                                            'original due date', \n",
    "                                            'work order'], as_index = False).sum()  \n",
    "                                            \n",
    "    sku_sheet_groupable['start_date'] = \"\"\n",
    "    sku_sheet_groupable['ending_date'] = \"\"\n",
    "\n",
    "    for machine, dic in data.items():\n",
    "        for bulk, dic_2 in dic.items():\n",
    "            for sequence, dates in dic_2.items():\n",
    "                sku_sheet_groupable.loc[(sku_sheet_groupable['machine code'] == machine) & \n",
    "                                (sku_sheet_groupable['sku'] == bulk) & \n",
    "                                (sku_sheet_groupable['sequence'] == sequence), 'start_date'] = dates[0]\n",
    "\n",
    "\n",
    "                sku_sheet_groupable.loc[(sku_sheet_groupable['machine code'] == machine) & \n",
    "                                (sku_sheet_groupable['sku'] == bulk) &\n",
    "                                (sku_sheet_groupable['sequence'] == sequence), 'ending_date'] = dates[1]    \n",
    "\n",
    "    sku_sheet_groupable = sku_sheet_groupable[['seed','machine', 'machine code', 'sku','start_date',\n",
    "                                               'ending_date','hours','order demand pounds', 'demand attained pounds', 'production demand pounds',\n",
    "                                               'order demand bags', 'production demand bags', 'demand attained bags', \n",
    "                                               'sku description', 'formula', 'inventory demand pounds', \n",
    "                                               'inventory demand bags', 'purchase order', 'sales order', 'original due date', \n",
    "                                               'entity', 'work order']]\n",
    "    \n",
    "    sku_sheet_not_groupable = sku_sheet_not_groupable[['seed','machine', 'machine code', 'sku','start_date',\n",
    "                                                       'ending_date','hours','order demand pounds', 'demand attained pounds', 'production demand pounds',\n",
    "                                                       'order demand bags', 'production demand bags', 'demand attained bags', \n",
    "                                                       'sku description', 'formula', 'inventory demand pounds', \n",
    "                                                       'inventory demand bags', 'purchase order', 'sales order', 'original due date', \n",
    "                                                       'entity', 'work order']]\n",
    "    \n",
    "    availables = sku_sheet_groupable['sku'].unique().tolist()\n",
    "    \n",
    "    sku_sheet_not_groupable.loc[(sku_sheet_not_groupable['machine'] == \"X165 TOTE LINE\") &\n",
    "                                (sku_sheet_not_groupable['sku'].isin(availables)) ,'purchase order'] = \"*\"\n",
    "                                  \n",
    "    sku_sheet_not_groupable.loc[(sku_sheet_not_groupable['machine'] == \"X165 TOTE LINE\") &\n",
    "                                (sku_sheet_not_groupable['sku'].isin(availables)) ,'work order'] = \"*\"\n",
    "    \n",
    "    sku_sheet_not_groupable.loc[(sku_sheet_not_groupable['machine'] == \"X165 TOTE LINE\") &\n",
    "                                (sku_sheet_not_groupable['sku'].isin(availables)) ,'sales order'] = \"*\"\n",
    "    \n",
    "  \n",
    "\n",
    "    final_df = pd.concat([sku_sheet_groupable, sku_sheet_not_groupable])\n",
    "    final_df.sort_values(by = [\"start_date\"], ascending = [True], inplace = True)\n",
    "    final_df.reset_index(drop = True, inplace = True)\n",
    "    final_df['sequence'] = [i for i in range(len(final_df))]\n",
    "\n",
    "    # Converts grouped extrusion model output into SAC format input\n",
    "    final_df['run'] = extruders.loc[0,\"run\"]\n",
    "\n",
    "    final_df.rename(columns={'machine code': 'Work_Center', 'sku': 'Item_Code', \"order demand pounds\": \"Order_Dem\",\n",
    "                 \"production demand pounds\": \"Prod_Dem\", \"demand attained pounds\": \"Att_Dem\", \n",
    "                 \"order demand bags\": \"Order_Bags\", \"production demand bags\": \"Prod_Bags\",\n",
    "                 \"demand attained bags\": \"Att_Bags\", \"hours\": \"Hours\", 'sequence': 'Machine_Sequence', 'start_date': 'start_time',\n",
    "                 'ending_date': 'ending_time', 'purchase order' : 'Purchase_Order',\n",
    "                 'work order' : 'Work_Order', 'sales order': 'Sales_Order', \n",
    "                 'original due date' : 'Original_Due_DATE', 'run':'Run'}, inplace=True)\n",
    "\n",
    "    final_df['Item_Code'] = final_df['Item_Code'].apply(str)\n",
    "\n",
    "    \n",
    "    final_df = final_df[['Work_Center', 'Item_Code', 'start_time', \n",
    "                   'ending_time', 'Machine_Sequence', 'Run', 'Purchase_Order', \n",
    "                   'Work_Order','Sales_Order','Original_Due_DATE', 'Att_Dem']].copy()\n",
    "\n",
    "    final_df['start_time'] = final_df['start_time'].astype(str)\n",
    "    final_df['ending_time'] = final_df['ending_time'].astype(str)\n",
    "    final_df['start_date'] = pd.to_datetime(final_df['start_time'].str.split().str[0]).dt.date\n",
    "    final_df['ending_date'] = pd.to_datetime(final_df['ending_time'].str.split().str[0]).dt.date\n",
    "    final_df['start_hour'] = pd.to_datetime(final_df['start_time'].str.split().str[1]).dt.time\n",
    "    final_df['ending_hour'] = pd.to_datetime(final_df['ending_time'].str.split().str[1]).dt.time\n",
    "\n",
    "    final_df['start_time'] = pd.to_datetime(final_df['start_time'].astype(str))\n",
    "    final_df['ending_time'] = pd.to_datetime(final_df['ending_time'].astype(str))\n",
    "\n",
    "    final_df[\"Version\"] = \"SIM\"\n",
    "\n",
    "    final_df['Process Date'] = week()\n",
    "    \n",
    "    final_df['Entity'] = 'CJ_Foods'\n",
    "    \n",
    "\n",
    "    final_df = final_df[['Work_Center', 'Item_Code', 'start_date', 'start_hour', 'start_time', 'ending_date',\n",
    "                         'ending_hour', 'ending_time', 'Original_Due_DATE', 'Machine_Sequence', 'Run', 'Process Date', 'Entity',\n",
    "                         'Version', 'Sales_Order', 'Purchase_Order', 'Work_Order']].copy()\n",
    "\n",
    "    \n",
    "    final_df[\"Version\"] = 'SIM'\n",
    "    final_df['Work_Order'].fillna('0', inplace=True)\n",
    "    final_df['Purchase_Order'].fillna('0', inplace=True)\n",
    "\n",
    "    final_df['Work_Order'] = [str(order).split(\".\")[0] for order in final_df['Work_Order']]\n",
    "\n",
    "    final_df['Original_Due_DATE'] = final_df['Original_Due_DATE'].astype(str).str.split().str[0]\n",
    "\n",
    "    final_df.name = 'GROUPED_PACKING_SAC'\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectToHANA():\n",
    "    #conecta a Hana\n",
    "    try:\n",
    "        conn = sqlalchemy.create_engine(\n",
    "            'hana://DBADMIN:HANAtest2908@8969f818-750f-468f-afff-3dc99a6e805b.hana.trial-us10.hanacloud.ondemand.com:443/?encrypt=true&validateCertificate=false').connect()\n",
    "    except Exception as e:\n",
    "        print('Connection failed! ' + str(e))\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Upload_Output_to_hana():\n",
    "    #sube la lista de tablas de SACA a Hana y pisa segun week y run\n",
    "\n",
    "    connection = connectToHANA()\n",
    "    lista_tablas_para_SAC = [extruders_output(), packlines_output(), unpacked_output(), grouped_extrusion_output(), grouped_packing_output()]\n",
    "    for table in lista_tablas_para_SAC:\n",
    "        try:\n",
    "            Run = table.loc[0,\"Run\"]\n",
    "            Process_Date = table.loc[0,\"Process Date\"]\n",
    "            connection.execute(f\"\"\"DELETE FROM \"DBADMIN\".\"{table.name}\" WHERE \"Process Date\" = '{Process_Date}' and \"Run\" = '{Run}'\"\"\")\n",
    "            print('Values deleted succesfully')\n",
    "            table.to_sql(table.name.lower(), con=connection, if_exists='append', index=False)\n",
    "            print(table.name + ' uploaded succesfully')\n",
    "        except Exception as e:\n",
    "            print(table.name +' failed to upload! ' + str(e))\n",
    "    connection.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyRr90otIVsyMnSYvxrxzq",
   "name": "Save output to hana.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
